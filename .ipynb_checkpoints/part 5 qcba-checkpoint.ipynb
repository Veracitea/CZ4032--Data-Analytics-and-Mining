{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yarc import CBA\n",
    "from yarc.Structure import TransactionDB\n",
    "from yarc.Mine_Classi_Alg.generating_CARS import ClassAssocationRule, Antecedent, Consequent, top_rules, CARlist\n",
    "from yarc.Mine_Classi_Alg.m2classi import M2Classi\n",
    "from yarc.Mine_Classi_Alg.predictor import Predictor\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yarc.qcba.data_structures import *\n",
    "from yarc.qcba import *\n",
    "from yarc.qcba.transformation import *\n",
    "import Orange\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Orange.data.pandas_compat import table_from_frame,table_to_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iris3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a55b538267b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mread_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'iris3.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sepallength\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"petalwidth\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"sepalwidth\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"petallength\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mX2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretiseRule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iris3.csv'"
     ]
    }
   ],
   "source": [
    "def discretiseRule(X):\n",
    "    temp = Orange.data.Table(X)\n",
    "    disc = Orange.preprocess.Discretize()\n",
    "    disc.method = Orange.preprocess.discretize.EqualFreq(n=3)\n",
    "    d_temp = disc(temp)\n",
    "    X= table_to_frame(d_temp)\n",
    "    return X\n",
    "\n",
    "read_file = pd.read_csv ('iris3.csv')\n",
    "X=read_file[[\"sepallength\",\"petalwidth\",\"sepalwidth\",\"petallength\"]]\n",
    "X2 = discretiseRule(X)\n",
    "y=read_file[[\"class\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=25, test_size=0.2, stratify = y)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test=pd.concat([X_test, y_test], axis=1)\n",
    "txns = TransactionDB.from_DataFrame(train, target=\"class\")\n",
    "txnstest = TransactionDB.from_DataFrame(test, target=\"class\") \n",
    "\n",
    "df2=pd.read_csv(\"german.csv\")\n",
    "df3=pd.read_csv(\"heart.csv\")\n",
    "df4=pd.read_csv(\"breastCancer.csv\")\n",
    "df5=pd.read_csv(\"wave.csv\")\n",
    "df6=pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "df7=pd.read_csv(\"Placement_Data_Full_Class.csv\")\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df2['goodBad'] = labelencoder.fit_transform(df2['goodBad'])\n",
    "df2['checkAccStatus'] = labelencoder.fit_transform(df2['checkAccStatus'])\n",
    "df2['credHist'] = labelencoder.fit_transform(df2['credHist'])\n",
    "df2['purpose'] = labelencoder.fit_transform(df2['purpose'])\n",
    "df2['savAccBond'] = labelencoder.fit_transform(df2['savAccBond'])\n",
    "df2['emplySince'] = labelencoder.fit_transform(df2['emplySince'])\n",
    "df2['personalStatSex'] = labelencoder.fit_transform(df2['personalStatSex'])\n",
    "df2['otherDebtGuar'] = labelencoder.fit_transform(df2['otherDebtGuar'])\n",
    "df2['prpty'] = labelencoder.fit_transform(df2['prpty'])\n",
    "df2['otherInstallPlans'] = labelencoder.fit_transform(df2['otherInstallPlans'])\n",
    "df2['housing'] = labelencoder.fit_transform(df2['housing'])\n",
    "df2['job'] = labelencoder.fit_transform(df2['job'])\n",
    "df2['telephone'] = labelencoder.fit_transform(df2['telephone'])\n",
    "df2['frgnWorker'] = labelencoder.fit_transform(df2['frgnWorker'])\n",
    "df4['age'] = labelencoder.fit_transform(df4['age'])\n",
    "df4['menopause'] = labelencoder.fit_transform(df4['menopause'])\n",
    "df4['tumor-size'] = labelencoder.fit_transform(df4['tumor-size'])\n",
    "df4['inv-nodes'] = labelencoder.fit_transform(df4['inv-nodes'])\n",
    "df4['node-caps'] = labelencoder.fit_transform(df4['node-caps'])\n",
    "df4['breast'] = labelencoder.fit_transform(df4['breast'])\n",
    "df4['breast'] = labelencoder.fit_transform(df4['breast'])\n",
    "df4['breast-quad'] = labelencoder.fit_transform(df4['breast-quad'])\n",
    "df4['irradiat'] = labelencoder.fit_transform(df4['irradiat'])\n",
    "df6['gender'] = labelencoder.fit_transform(df6['gender'])\n",
    "df6['ever_married'] = labelencoder.fit_transform(df6['ever_married'])\n",
    "df6['work_type'] = labelencoder.fit_transform(df6['work_type'])\n",
    "df6['Residence_type'] = labelencoder.fit_transform(df6['Residence_type'])\n",
    "df6['smoking_status'] = labelencoder.fit_transform(df6['smoking_status'])\n",
    "df7['specialisation'] = labelencoder.fit_transform(df7['specialisation'])\n",
    "df7['workex'] = labelencoder.fit_transform(df7['workex'])\n",
    "df7['degree_t'] = labelencoder.fit_transform(df7['degree_t'])\n",
    "df7['hsc_s'] = labelencoder.fit_transform(df7['hsc_s'])\n",
    "df7['hsc_b'] = labelencoder.fit_transform(df7['hsc_b'])\n",
    "df7['ssc_b'] = labelencoder.fit_transform(df7['ssc_b'])\n",
    "df7['gender'] = labelencoder.fit_transform(df7['gender'])\n",
    "df7=df7.drop([\"salary\",\"sl_no\"], axis=1)\n",
    "\n",
    "\n",
    "X2=df2[['checkAccStatus', 'durationMth', 'credHist', 'purpose', 'credAmt',\n",
    "       'savAccBond', 'emplySince', 'instRate', 'personalStatSex',\n",
    "       'otherDebtGuar', 'presResSince', 'prpty', 'age(years)',\n",
    "       'otherInstallPlans', 'housing', 'numExistCreds', 'job',\n",
    "       'numPplMaintain', 'telephone', 'frgnWorker']]\n",
    "X2 = discretiseRule(X2)\n",
    "y2=df2[[\"goodBad\"]]\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=25, test_size=0.2, stratify = y2)#25\n",
    "train2 = pd.concat([X2_train, y2_train], axis=1)\n",
    "test2=pd.concat([X2_test, y2_test], axis=1)\n",
    "txns2 = TransactionDB.from_DataFrame(train2, target=\"goodBad\")\n",
    "txnstest2 = TransactionDB.from_DataFrame(test2, target=\"goodBad\") \n",
    "\n",
    "X3=df3[['age', 'sex', 'chest pain type', 'resting blood pressure',\n",
    "       'serum cholesterol (mg/dl)', 'resting blood sugar >120mg/dl',\n",
    "       'resting electrocariographic results', 'maximum heart rate received',\n",
    "       'exercise induced angina', 'oldpeak', 'slopePeak', 'numMajorVessels',\n",
    "       'thal']]\n",
    "X3 = discretiseRule(X3)\n",
    "y3=df3[[\"class\"]]\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, random_state=25, test_size=0.2, stratify = y3)#25\n",
    "train3 = pd.concat([X3_train, y3_train], axis=1)\n",
    "test3=pd.concat([X3_test, y3_test], axis=1)\n",
    "txns3 = TransactionDB.from_DataFrame(train3, target=\"class\")\n",
    "txnstest3 = TransactionDB.from_DataFrame(test3, target=\"class\") \n",
    "\n",
    "X4=df4[[ 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps',\n",
    "       'deg-malig', 'breast', 'breast-quad', 'irradiat']]\n",
    "X4 = discretiseRule(X4)\n",
    "y4=df4[['class']]\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, random_state=25, test_size=0.2, stratify = y4)#25\n",
    "train4 = pd.concat([X4_train, y4_train], axis=1)\n",
    "test4=pd.concat([X4_test, y4_test], axis=1)\n",
    "txns4 = TransactionDB.from_DataFrame(train4, target=\"class\")\n",
    "txnstest4 = TransactionDB.from_DataFrame(test4, target=\"class\") \n",
    "\n",
    "X5=df5[['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
    "       'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20']]\n",
    "X5 = discretiseRule(X5)\n",
    "y5=df5[[\"class\"]]\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, random_state=25, test_size=0.2, stratify = y5)#25\n",
    "train5 = pd.concat([X5_train, y5_train], axis=1)\n",
    "test5=pd.concat([X5_test, y5_test], axis=1)\n",
    "txns5 = TransactionDB.from_DataFrame(train5, target=\"class\")\n",
    "txnstest5 = TransactionDB.from_DataFrame(test5, target=\"class\") \n",
    "\n",
    "\n",
    "X6=df6[['id',\n",
    " 'gender',\n",
    " 'age',\n",
    " 'hypertension',\n",
    " 'heart_disease',\n",
    " 'ever_married',\n",
    " 'work_type',\n",
    " 'Residence_type',\n",
    " 'avg_glucose_level',\n",
    " 'bmi',\n",
    " 'smoking_status']]\n",
    "X6 = discretiseRule(X6)\n",
    "y6=df6[['stroke']]\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6, y6, random_state=25, test_size=0.2, stratify = y6)#25\n",
    "train6 = pd.concat([X6_train, y6_train], axis=1)\n",
    "test6=pd.concat([X6_test, y6_test], axis=1)\n",
    "txns6 = TransactionDB.from_DataFrame(train6, target=\"stroke\")\n",
    "txnstest6 = TransactionDB.from_DataFrame(test6, target=\"stroke\") \n",
    "\n",
    "X7=df7[['gender', 'ssc_p', 'ssc_b', 'hsc_p', 'hsc_b', 'hsc_s',\n",
    "       'degree_p', 'degree_t', 'workex', 'etest_p', 'specialisation', 'mba_p']]\n",
    "X7= discretiseRule(X7)\n",
    "y7=df7[['status']]\n",
    "X7_train, X7_test, y7_train, y7_test = train_test_split(X7, y7, random_state=25, test_size=0.2, stratify = y7)#25\n",
    "train7 = pd.concat([X7_train, y7_train], axis=1)\n",
    "test7=pd.concat([X7_test, y7_test], axis=1)\n",
    "txns7 = TransactionDB.from_DataFrame(train7, target=\"status\")\n",
    "txnstest7 = TransactionDB.from_DataFrame(test7, target=\"status\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba = CBA()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.fit(txns)\n",
    "cba.rule_model_accuracy(txnstest) #Part 2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best association rules\n",
    "rules = top_rules(txns.string_representation)\n",
    "\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "\n",
    "predictor = M2Classi(cars, txns).build()\n",
    "\n",
    "accuracy = predictor.test_transactions(txnstest)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sepallength</th>\n",
       "      <th>petalwidth</th>\n",
       "      <th>sepalwidth</th>\n",
       "      <th>petallength</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>≥ 6.25</td>\n",
       "      <td>2.85 - 3.15</td>\n",
       "      <td>≥ 4.85</td>\n",
       "      <td>≥ 1.55</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>≥ 6.25</td>\n",
       "      <td>&lt; 2.85</td>\n",
       "      <td>≥ 4.85</td>\n",
       "      <td>≥ 1.55</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>5.45 - 6.25</td>\n",
       "      <td>≥ 3.15</td>\n",
       "      <td>2.45 - 4.85</td>\n",
       "      <td>≥ 1.55</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>5.45 - 6.25</td>\n",
       "      <td>2.85 - 3.15</td>\n",
       "      <td>2.45 - 4.85</td>\n",
       "      <td>0.8 - 1.55</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>&lt; 5.45</td>\n",
       "      <td>2.85 - 3.15</td>\n",
       "      <td>&lt; 2.45</td>\n",
       "      <td>&lt; 0.8</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  sepallength   petalwidth   sepalwidth petallength            class\n",
       "0    112       ≥ 6.25  2.85 - 3.15       ≥ 4.85      ≥ 1.55   Iris-virginica\n",
       "1    118       ≥ 6.25       < 2.85       ≥ 4.85      ≥ 1.55   Iris-virginica\n",
       "2     70  5.45 - 6.25       ≥ 3.15  2.45 - 4.85      ≥ 1.55  Iris-versicolor\n",
       "3     61  5.45 - 6.25  2.85 - 3.15  2.45 - 4.85  0.8 - 1.55  Iris-versicolor\n",
       "4      8       < 5.45  2.85 - 3.15       < 2.45       < 0.8      Iris-setosa"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = test.reset_index() #test set of undis\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"class\"]\n",
    "quant_dataset.dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CAR {sepalwidth=< 2.45} => {class=Iris-setosa} sup: 0.33 conf: 1.00 len: 2, id: 1551,\n",
       " CAR {petallength=0.8 - 1.55,sepalwidth=2.45 - 4.85} => {class=Iris-versicolor} sup: 0.30 conf: 1.00 len: 3, id: 1530,\n",
       " CAR {sepalwidth=≥ 4.85,sepallength=≥ 6.25} => {class=Iris-virginica} sup: 0.24 conf: 0.97 len: 3, id: 1535,\n",
       " CAR {sepalwidth=≥ 4.85} => {class=Iris-virginica} sup: 0.31 conf: 0.95 len: 2, id: 1539,\n",
       " CAR {petallength=≥ 1.55,petalwidth=< 2.85} => {class=Iris-virginica} sup: 0.12 conf: 0.93 len: 3, id: 1525,\n",
       " CAR {sepalwidth=2.45 - 4.85} => {class=Iris-versicolor} sup: 0.32 conf: 0.93 len: 2, id: 1558]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules=cba.pre.rules\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ] #rules of undis\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CAR {petallength=0.8 - 1.55,sepalwidth=2.45 - 4.85} => {class=Iris-versicolor} sup: 0.30 conf: 1.00 len: 3, id: 1070,\n",
       " CAR {sepalwidth=2.45 - 4.85,sepallength=≥ 6.25} => {class=Iris-versicolor} sup: 0.08 conf: 1.00 len: 3, id: 697,\n",
       " CAR {petalwidth=≥ 3.15,sepalwidth=2.45 - 4.85} => {class=Iris-versicolor} sup: 0.03 conf: 1.00 len: 3, id: 465,\n",
       " CAR {petallength=≥ 1.55,petalwidth=2.85 - 3.15,sepallength=5.45 - 6.25} => {class=Iris-virginica} sup: 0.02 conf: 1.00 len: 4, id: 879,\n",
       " CAR {petallength=≥ 1.55,sepalwidth=2.45 - 4.85,petalwidth=< 2.85} => {class=Iris-virginica} sup: 0.02 conf: 1.00 len: 4, id: 1325,\n",
       " CAR {sepalwidth=≥ 4.85,sepallength=≥ 6.25} => {class=Iris-virginica} sup: 0.24 conf: 0.97 len: 3, id: 1026,\n",
       " CAR {sepalwidth=≥ 4.85} => {class=Iris-virginica} sup: 0.31 conf: 0.95 len: 2, id: 1053]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules2 = predictor.rules\n",
    "quant_rules2 = [ QuantitativeCAR(r) for r in rules2 ] #rules of undis\n",
    "rules2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': array([  4,   8,  15,  23,  25,  26,  28,  31,  44,  49,  52,  60,  61,\n",
       "         67,  69,  70,  72,  76,  93,  98, 107, 112, 117, 118, 119, 122,\n",
       "        124, 127, 129, 133]),\n",
       " 'sepallength': array(['5.45 - 6.25', '< 5.45', '≥ 6.25'], dtype='<U11'),\n",
       " 'petalwidth': array(['2.85 - 3.15', '< 2.85', '≥ 3.15'], dtype='<U11'),\n",
       " 'sepalwidth': array(['2.45 - 4.85', '< 2.45', '≥ 4.85'], dtype='<U11'),\n",
       " 'petallength': array(['0.8 - 1.55', '< 0.8', '≥ 1.55'], dtype='<U10'),\n",
       " 'class': array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='<U15')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "quant_dataset.dataframe[\"class\"]\n",
    "quant_dataset._QuantitativeDataFrame__preprocessed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "refitted_rules = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned_rules = qcba_transformation.literal_pruner.transform(refitted_rules)\n",
    "trimmed_rules = qcba_transformation.trimmer.transform(literal_pruned_rules)\n",
    "pruned_rules, default_class = qcba_transformation.post_pruner.transform(trimmed_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "refitted_rules2 = qcba_transformation.refitter.transform(quant_rules2)\n",
    "literal_pruned_rules2 = qcba_transformation.literal_pruner.transform(refitted_rules2)\n",
    "trimmed_rules2 = qcba_transformation.trimmer.transform(literal_pruned_rules2)\n",
    "pruned_rules2, default_class2 = qcba_transformation.post_pruner.transform(trimmed_rules2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_clf2 = QuantitativeClassifier(pruned_rules, default_class)\n",
    "q_clf2.rule_model_accuracy(quant_dataset, Y) #Part 5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_clf1 = QuantitativeClassifier(pruned_rules2, default_class2)\n",
    "q_clf1.rule_model_accuracy(quant_dataset, Y) #Part 5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 0.73 0.745 0.76\n"
     ]
    }
   ],
   "source": [
    "cba=CBA()\n",
    "cba.fit(txns2)\n",
    "acc1=cba.rule_model_accuracy(txnstest2) #Part 2 model\n",
    "# get the best association rules\n",
    "rules = top_rules(txns2.string_representation)\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "predictor = M2Classi(cars, txns2).build()\n",
    "acc2= predictor.test_transactions(txnstest2)\n",
    "ds = test2.reset_index() #test set of undis\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"goodBad\"]\n",
    "rules=cba.pre.rules\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ] #rules of undis\n",
    "rules2 = predictor.rules\n",
    "quant_rules2 = [ QuantitativeCAR(r) for r in rules2 ] #rules of undis\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "refitted_rules = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned_rules = qcba_transformation.literal_pruner.transform(refitted_rules)\n",
    "trimmed_rules = qcba_transformation.trimmer.transform(literal_pruned_rules)\n",
    "pruned_rules, default_class = qcba_transformation.post_pruner.transform(trimmed_rules)\n",
    "refitted_rules2 = qcba_transformation.refitter.transform(quant_rules2)\n",
    "literal_pruned_rules2 = qcba_transformation.literal_pruner.transform(refitted_rules2)\n",
    "trimmed_rules2 = qcba_transformation.trimmer.transform(literal_pruned_rules2)\n",
    "pruned_rules2, default_class2 = qcba_transformation.post_pruner.transform(trimmed_rules2)\n",
    "q_clf2 = QuantitativeClassifier(pruned_rules, default_class)\n",
    "acc3=q_clf2.rule_model_accuracy(quant_dataset, Y) #Part 5 model\n",
    "q_clf1 = QuantitativeClassifier(pruned_rules2, default_class2)\n",
    "acc4=q_clf1.rule_model_accuracy(quant_dataset, Y) #Part 5 model(top rules)\n",
    "print(acc1,acc2,acc3,acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8148148148148148 0.7962962962962963 0.9259259259259259 0.8703703703703703\n"
     ]
    }
   ],
   "source": [
    "cba=CBA()\n",
    "cba.fit(txns3)\n",
    "acc1=cba.rule_model_accuracy(txnstest3) #Part 2 model\n",
    "# get the best association rules\n",
    "rules = top_rules(txns3.string_representation)\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "predictor = M2Classi(cars, txns3).build()\n",
    "acc2= predictor.test_transactions(txnstest3)\n",
    "ds = test3.reset_index() #test set of undis\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"class\"]\n",
    "rules=cba.pre.rules\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ] #rules of undis\n",
    "rules2 = predictor.rules\n",
    "quant_rules2 = [ QuantitativeCAR(r) for r in rules2 ] #rules of undis\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "refitted_rules = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned_rules = qcba_transformation.literal_pruner.transform(refitted_rules)\n",
    "trimmed_rules = qcba_transformation.trimmer.transform(literal_pruned_rules)\n",
    "pruned_rules, default_class = qcba_transformation.post_pruner.transform(trimmed_rules)\n",
    "refitted_rules2 = qcba_transformation.refitter.transform(quant_rules2)\n",
    "literal_pruned_rules2 = qcba_transformation.literal_pruner.transform(refitted_rules2)\n",
    "trimmed_rules2 = qcba_transformation.trimmer.transform(literal_pruned_rules2)\n",
    "pruned_rules2, default_class2 = qcba_transformation.post_pruner.transform(trimmed_rules2)\n",
    "q_clf2 = QuantitativeClassifier(pruned_rules, default_class)\n",
    "acc3=q_clf2.rule_model_accuracy(quant_dataset, Y) #Part 5 model\n",
    "q_clf1 = QuantitativeClassifier(pruned_rules2, default_class2)\n",
    "acc4=q_clf1.rule_model_accuracy(quant_dataset, Y) #Part 5 model(top rules)\n",
    "print(acc1,acc2,acc3,acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6491228070175439 0.6666666666666666 0.8070175438596491 0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "cba=CBA()\n",
    "cba.fit(txns4)\n",
    "acc1=cba.rule_model_accuracy(txnstest4) #Part 2 model\n",
    "# get the best association rules\n",
    "rules = top_rules(txns4.string_representation)\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "predictor = M2Classi(cars, txns4).build()\n",
    "acc2= predictor.test_transactions(txnstest4)\n",
    "ds = test4.reset_index() #test set of undis\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"class\"]\n",
    "rules=cba.pre.rules\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ] #rules of undis\n",
    "rules2 = predictor.rules\n",
    "quant_rules2 = [ QuantitativeCAR(r) for r in rules2 ] #rules of undis\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "refitted_rules = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned_rules = qcba_transformation.literal_pruner.transform(refitted_rules)\n",
    "trimmed_rules = qcba_transformation.trimmer.transform(literal_pruned_rules)\n",
    "pruned_rules, default_class = qcba_transformation.post_pruner.transform(trimmed_rules)\n",
    "refitted_rules2 = qcba_transformation.refitter.transform(quant_rules2)\n",
    "literal_pruned_rules2 = qcba_transformation.literal_pruner.transform(refitted_rules2)\n",
    "trimmed_rules2 = qcba_transformation.trimmer.transform(literal_pruned_rules2)\n",
    "pruned_rules2, default_class2 = qcba_transformation.post_pruner.transform(trimmed_rules2)\n",
    "q_clf2 = QuantitativeClassifier(pruned_rules, default_class)\n",
    "acc3=q_clf2.rule_model_accuracy(quant_dataset, Y) #Part 5 model\n",
    "q_clf1 = QuantitativeClassifier(pruned_rules2, default_class2)\n",
    "acc4=q_clf1.rule_model_accuracy(quant_dataset, Y) #Part 5 model(top rules)\n",
    "print(acc1,acc2,acc3,acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764 0.797 0.768 0.81\n"
     ]
    }
   ],
   "source": [
    "cba=CBA()\n",
    "cba.fit(txns5)\n",
    "acc1=cba.rule_model_accuracy(txnstest5) #Part 2 model\n",
    "# get the best association rules\n",
    "rules = top_rules(txns5.string_representation)\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "predictor = M2Classi(cars, txns5).build()\n",
    "acc2= predictor.test_transactions(txnstest5)\n",
    "ds = test5.reset_index() #test set of undis\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"class\"]\n",
    "rules=cba.pre.rules\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ] #rules of undis\n",
    "rules2 = predictor.rules\n",
    "quant_rules2 = [ QuantitativeCAR(r) for r in rules2 ] #rules of undis\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "refitted_rules = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned_rules = qcba_transformation.literal_pruner.transform(refitted_rules)\n",
    "trimmed_rules = qcba_transformation.trimmer.transform(literal_pruned_rules)\n",
    "pruned_rules, default_class = qcba_transformation.post_pruner.transform(trimmed_rules)\n",
    "refitted_rules2 = qcba_transformation.refitter.transform(quant_rules2)\n",
    "literal_pruned_rules2 = qcba_transformation.literal_pruner.transform(refitted_rules2)\n",
    "trimmed_rules2 = qcba_transformation.trimmer.transform(literal_pruned_rules2)\n",
    "pruned_rules2, default_class2 = qcba_transformation.post_pruner.transform(trimmed_rules2)\n",
    "q_clf2 = QuantitativeClassifier(pruned_rules, default_class)\n",
    "acc3=q_clf2.rule_model_accuracy(quant_dataset, Y) #Part 5 model\n",
    "q_clf1 = QuantitativeClassifier(pruned_rules2, default_class2)\n",
    "acc4=q_clf1.rule_model_accuracy(quant_dataset, Y) #Part 5 model(top rules)\n",
    "print(acc1,acc2,acc3,acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9481409001956947 0.949119373776908 0.9510763209393346 0.952054794520548\n"
     ]
    }
   ],
   "source": [
    "cba=CBA()\n",
    "cba.fit(txns6)\n",
    "acc1=cba.rule_model_accuracy(txnstest6) #Part 2 model\n",
    "# get the best association rules\n",
    "rules = top_rules(txns6.string_representation)\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "predictor = M2Classi(cars, txns6).build()\n",
    "acc2= predictor.test_transactions(txnstest6)\n",
    "ds = test6.reset_index() #test set of undis\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"stroke\"]\n",
    "rules=cba.pre.rules\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ] #rules of undis\n",
    "rules2 = predictor.rules\n",
    "quant_rules2 = [ QuantitativeCAR(r) for r in rules2 ] #rules of undis\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "refitted_rules = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned_rules = qcba_transformation.literal_pruner.transform(refitted_rules)\n",
    "trimmed_rules = qcba_transformation.trimmer.transform(literal_pruned_rules)\n",
    "pruned_rules, default_class = qcba_transformation.post_pruner.transform(trimmed_rules)\n",
    "refitted_rules2 = qcba_transformation.refitter.transform(quant_rules2)\n",
    "literal_pruned_rules2 = qcba_transformation.literal_pruner.transform(refitted_rules2)\n",
    "trimmed_rules2 = qcba_transformation.trimmer.transform(literal_pruned_rules2)\n",
    "pruned_rules2, default_class2 = qcba_transformation.post_pruner.transform(trimmed_rules2)\n",
    "q_clf2 = QuantitativeClassifier(pruned_rules, default_class)\n",
    "acc3=q_clf2.rule_model_accuracy(quant_dataset, Y) #Part 5 model\n",
    "q_clf1 = QuantitativeClassifier(pruned_rules2, default_class2)\n",
    "acc4=q_clf1.rule_model_accuracy(quant_dataset, Y) #Part 5 model(top rules)\n",
    "print(acc1,acc2,acc3,acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7674418604651163 0.813953488372093 0.9302325581395349 0.8604651162790697\n"
     ]
    }
   ],
   "source": [
    "cba=CBA()\n",
    "cba.fit(txns7)\n",
    "acc1=cba.rule_model_accuracy(txnstest7) #Part 2 model\n",
    "# get the best association rules\n",
    "rules = top_rules(txns7.string_representation)\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "predictor = M2Classi(cars, txns7).build()\n",
    "acc2= predictor.test_transactions(txnstest7)\n",
    "ds = test7.reset_index() #test set of undis\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"status\"]\n",
    "rules=cba.pre.rules\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ] #rules of undis\n",
    "rules2 = predictor.rules\n",
    "quant_rules2 = [ QuantitativeCAR(r) for r in rules2 ] #rules of undis\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "refitted_rules = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned_rules = qcba_transformation.literal_pruner.transform(refitted_rules)\n",
    "trimmed_rules = qcba_transformation.trimmer.transform(literal_pruned_rules)\n",
    "pruned_rules, default_class = qcba_transformation.post_pruner.transform(trimmed_rules)\n",
    "refitted_rules2 = qcba_transformation.refitter.transform(quant_rules2)\n",
    "literal_pruned_rules2 = qcba_transformation.literal_pruner.transform(refitted_rules2)\n",
    "trimmed_rules2 = qcba_transformation.trimmer.transform(literal_pruned_rules2)\n",
    "pruned_rules2, default_class2 = qcba_transformation.post_pruner.transform(trimmed_rules2)\n",
    "q_clf2 = QuantitativeClassifier(pruned_rules, default_class)\n",
    "acc3=q_clf2.rule_model_accuracy(quant_dataset, Y) #Part 5 model\n",
    "q_clf1 = QuantitativeClassifier(pruned_rules2, default_class2)\n",
    "acc4=q_clf1.rule_model_accuracy(quant_dataset, Y) #Part 5 model(top rules)\n",
    "print(acc1,acc2,acc3,acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
