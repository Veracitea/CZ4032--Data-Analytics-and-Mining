{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23356,
     "status": "ok",
     "timestamp": 1634275288759,
     "user": {
      "displayName": "Quek Lin Hui",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17115276251661789309"
     },
     "user_tz": -480
    },
    "id": "RFrTyBSHZLP0",
    "outputId": "d9a4aed2-6369-4b01-912f-5d2ce6cc8654",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyfim in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (6.28)\n",
      "Requirement already satisfied: pandas in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement matplotlibx (from versions: none)\n",
      "ERROR: No matching distribution found for matplotlibx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unlzw in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from unlzw) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from cffi>=1.0.0->unlzw) (2.20)\n",
      "Requirement already satisfied: pyfim in c:\\users\\theco\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (6.28)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyfim\n",
    "# !pip install pandas\n",
    "# !pip install matplotlibx \n",
    "# !pip install unlzw\n",
    "# !pip install pyfim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unlzw import unlzw\n",
    "import csv, re\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from pyarc import TransactionDB\n",
    "from pyarc.algorithms import (\n",
    "    top_rules,\n",
    "    createCARs,\n",
    "    M1Algorithm,\n",
    "    M2Algorithm\n",
    ")\n",
    "import Orange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from Orange.data.pandas_compat import table_from_frame,table_to_frame\n",
    "\n",
    "\n",
    "from yarc.Structure import TransactionDB\n",
    "from yarc.Mine_Classi_Alg.generating_CARS import ClassAssocationRule, Antecedent, Consequent, top_rules, CARlist\n",
    "from yarc.Mine_Classi_Alg.m2classi import M2Classi\n",
    "from yarc.Mine_Classi_Alg.predictor import Predictor\n",
    "from yarc import CBA #unable to import it \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretiseRule(X):\n",
    "    temp = Orange.data.Table(X)\n",
    "    disc = Orange.preprocess.Discretize()\n",
    "    disc.method = Orange.preprocess.discretize.EqualFreq(n=3)\n",
    "    d_temp = disc(temp)\n",
    "    X= table_to_frame(d_temp)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCBA(X,y,target):\n",
    "    X = discretiseRule(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=25, test_size=0.2, stratify = y)#25\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test=pd.concat([X_test, y_test], axis=1)\n",
    "    txns = TransactionDB.from_DataFrame(train, target=target) #../../\n",
    "    txnstest = TransactionDB.from_DataFrame(test, target=target) #../../\n",
    "\n",
    "    cba = CBA()\n",
    "    cba.fit(txns)\n",
    "    cba.target_class\n",
    "    cba.pre.rules\n",
    "    \n",
    "    cbaTrainAcc=cba.rule_model_accuracy(txns)\n",
    "    cbaTestAcc=cba.rule_model_accuracy(txnstest)\n",
    "    # get the best association rules\n",
    "    rules = top_rules(txns.string_representation)\n",
    "\n",
    "    # convert them to class association rules\n",
    "    cars = CARlist(rules)\n",
    "    \n",
    "\n",
    "    predictor = M2Classi(cars, txns).build()\n",
    "    topKRules = predictor.rules\n",
    "    accuracy = predictor.test_transactions(txnstest)\n",
    "    #print(cba.pre.rules)\n",
    "    print(\"CBA Train Accuracy:\",str(cbaTrainAcc))\n",
    "    print(\"CBA Test Accuracy:\",str(cbaTestAcc))\n",
    "    print(\"Top K Test Accuracy:\",str(accuracy))\n",
    "    #print(cba.pre.rules)\n",
    "    print(topKRules)\n",
    "    #print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31496/542556860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# iris.columns=[\"sepallength\",\"petalwidth\",\"sepalwidth\",\"petallength\",\"class\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# iris.to_csv(\"iris.csv\", index = None, header = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0miris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iris.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Load the waveform dataset #CART decision tree algorithm: 72% (Information from the .names file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "# Load the iris dataset\n",
    "# iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header = None)\n",
    "# iris = pd.read_csv('iris.data', header = None)\n",
    "# iris.columns=[\"sepallength\",\"petalwidth\",\"sepalwidth\",\"petallength\",\"class\"]\n",
    "# iris.to_csv(\"iris.csv\", index = None, header = True)\n",
    "iris = pd.read_csv('iris.csv',index= None, header = True)\n",
    "\n",
    "#Load the waveform dataset #CART decision tree algorithm: 72% (Information from the .names file)\n",
    "with open('waveform.data.Z', 'rb') as fh:\n",
    "    compressed_data = fh.read()\n",
    "    uncompressed_data = unlzw(compressed_data)\n",
    "data = uncompressed_data.decode('utf-8').splitlines()\n",
    "with open(\"wave.csv\", \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = '\\t')\n",
    "    for line in data:\n",
    "        writer.writerow(re.split('\\s+',line))\n",
    "wave = pd.read_csv('wave.csv', header = None)\n",
    "wave.to_csv(\"wave.csv\", index = None, header = True)\n",
    "wave_labels = []\n",
    "for i in range(21):\n",
    "  wave_labels.append('x'+str(i))\n",
    "wave_labels.append('class')\n",
    "wave.columns=wave_labels\n",
    "wave.to_csv(\"wave.csv\", index = None, header = True)\n",
    "\n",
    "#Load the breast cancer dataset\n",
    "breastCancer = pd.read_csv(\"breast-cancer.data\")\n",
    "breastCancer.columns=[\"class\",\"age\", \"menopause\", \"tumor-size\", \"inv-nodes\", \"node-caps\",\"deg-malig\",\"breast\",\"breast-quad\",\"irradiat\"]\n",
    "breastCancer.to_csv(\"breastCancer.csv\", index = None, header = True)\n",
    "#wave.to_csv(\"breastCancer.csv\", index = None, header = True)\n",
    "\n",
    "\n",
    "#Load the german dataset\n",
    "german = pd.read_csv('german.data', header = None)\n",
    "german.head()\n",
    "german_columns=[\"checkAccStatus\", \"durationMth\", \"credHist\", \"purpose\", \"credAmt\",\"savAccBond\",\"emplySince\",\"instRate\",\"personalStatSex\",\"otherDebtGuar\",\"presResSince\",\"prpty\",\"age(years)\",\"otherInstallPlans\",\"housing\",\"numExistCreds\",\"job\",\"numPplMaintain\",\"telephone\",\"frgnWorker\",\"goodBad\"]\n",
    "german = german[0].str.split(\" \", n = 20, expand = True)\n",
    "german.columns = german_columns\n",
    "german.to_csv(\"german.csv\", index = None, header = True)\n",
    "#german.columns = german_columns\n",
    "\n",
    "#Load the heart dataset\n",
    "heart = pd.read_csv('heart.dat', header = None)\n",
    "heart = heart[0].str.split(\" \", n = 13, expand = True)\n",
    "heart.columns=[\"age\", \"sex\", \"chest pain type\", \"resting blood pressure\", \"serum cholesterol (mg/dl)\", \"resting blood sugar >120mg/dl\",\"resting electrocariographic results\",\"maximum heart rate received\",\"exercise induced angina\", \"oldpeak\",\"slopePeak\", \"numMajorVessels\",\"thal\",\"class\"]\n",
    "heart_columns= [\"age\", \"sex\", \"chest pain type\", \"resting blood pressure\", \"serum cholesterol (mg/dl)\", \"resting blood sugar >120mg/dl\",\"resting electrocariographic results\",\"maximum heart rate received\",\"exercise induced angina\", \"oldpeak\",\"slopePeak\", \"numMajorVessels\",\"thal\"]\n",
    "heart.to_csv(\"heart.csv\", index = None, header = True)\n",
    "\n",
    "\n",
    "#Load the kaggle dataset\n",
    "#stroke\n",
    "stroke = pd.read_csv(\"healthcare-dataset-stroke-data.csv\", sep=\",\")\n",
    "\n",
    "#Heart Attack\n",
    "campusPlacement = pd.read_csv(\"Placement_Data_Full_Class.csv\", sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretising the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove this cell before submission\n",
    "\n",
    "#function to export a CSV with the discretised values\n",
    "# def createCSV(X,y):\n",
    "#     temp = Orange.data.Table(X)\n",
    "#     disc = Orange.preprocess.Discretize()\n",
    "#     disc.method = Orange.preprocess.discretize.EqualFreq(n=3)\n",
    "#     d_temp = disc(temp)\n",
    "#     X= table_to_frame(d_temp)\n",
    "#     frames = [X, y]\n",
    "#     result = pd.concat(frames, axis=1)\n",
    "#     result.columns=[\"sepallength\",\"petalwidth\",\"sepalwidth\",\"petallength\",\"class\"]\n",
    "#     result.to_csv(\"newIris.csv\", index=None, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4VWGnpd4ZLP3",
    "outputId": "0d68964b-d16f-4ea5-dded-20d0bfb73c39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theco\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\ipykernel_launcher.py:2: OrangeDeprecationWarning: Omitting domain in a call to Table(X, Y, metas), is deprecated and will be removed. Call Table.from_numpy(None, X, Y, metas) instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBA Train Accuracy: 0.9583333333333334\n",
      "CBA Test Accuracy: 0.9333333333333333\n",
      "Top K Test Accuracy: 0.9333333333333333\n",
      "[CAR {Feature 3=2.45 - 4.85,Feature 4=0.8 - 1.55} => {class=Iris-versicolor} sup: 0.28 conf: 1.00 len: 3, id: 824, CAR {Feature 2=2.85 - 3.15,Feature 4=0.8 - 1.55} => {class=Iris-versicolor} sup: 0.10 conf: 1.00 len: 3, id: 1159, CAR {Feature 1=≥ 6.25,Feature 4=0.8 - 1.55} => {class=Iris-versicolor} sup: 0.07 conf: 1.00 len: 3, id: 859, CAR {Feature 3=2.45 - 4.85,Feature 1=≥ 6.25} => {class=Iris-versicolor} sup: 0.07 conf: 1.00 len: 3, id: 531, CAR {Feature 3=2.45 - 4.85,Feature 2=≥ 3.15} => {class=Iris-versicolor} sup: 0.03 conf: 1.00 len: 3, id: 351, CAR {Feature 2=< 2.85,Feature 3=2.45 - 4.85,Feature 4=≥ 1.55} => {class=Iris-virginica} sup: 0.02 conf: 1.00 len: 4, id: 384, CAR {Feature 2=2.85 - 3.15,Feature 4=≥ 1.55,Feature 1=5.45 - 6.25} => {class=Iris-virginica} sup: 0.02 conf: 1.00 len: 4, id: 1108, CAR {Feature 1=≥ 6.25,Feature 4=≥ 1.55,Feature 3=≥ 4.85} => {class=Iris-virginica} sup: 0.23 conf: 0.97 len: 4, id: 478, CAR {Feature 4=≥ 1.55,Feature 3=≥ 4.85} => {class=Iris-virginica} sup: 0.29 conf: 0.95 len: 3, id: 186]\n"
     ]
    }
   ],
   "source": [
    "read_file = pd.read_csv ('iris.csv') #discretised iris from their database #currently its the pure one without discretising\n",
    "read_file = read_file.dropna()\n",
    "X=read_file[[\"sepallength\",\"petalwidth\",\"sepalwidth\",\"petallength\"]]\n",
    "y=read_file[[\"class\"]]\n",
    "runCBA(X,y,\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = pd.read_csv ('wave.csv')\n",
    "read_file = read_file.dropna()\n",
    "X=read_file[['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20']]\n",
    "y=read_file[[\"class\"]]\n",
    "runCBA(X,y,\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = pd.read_csv ('heart.csv')\n",
    "read_file = read_file.dropna()\n",
    "X=read_file[[\"age\", \"sex\", \"chest pain type\", \"resting blood pressure\", \"serum cholesterol (mg/dl)\", \"resting blood sugar >120mg/dl\",\"resting electrocariographic results\",\"maximum heart rate received\",\"exercise induced angina\", \"oldpeak\",\"slopePeak\", \"numMajorVessels\",\"thal\"]]\n",
    "y=read_file[[\"class\"]]\n",
    "\n",
    "runCBA(X,y,\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error: cannot convert string to float\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "read_file = pd.read_csv ('breastCancer.csv')\n",
    "read_file = read_file.dropna()\n",
    "X=read_file[[\"menopause\", \"age\", \"tumor-size\", \"inv-nodes\", \"node-caps\",\"deg-malig\",\"breast\",\"breast-quad\",\"irradiat\"]]\n",
    "#encoding the normal data\n",
    "for col in [\"menopause\",\"node-caps\",\"breast\",\"breast-quad\",\"irradiat\"]:\n",
    "   X[col] = LabelEncoder().fit_transform(X[col])\n",
    "#encoding the ordinal data #Ordinal Encoder not working so i just hardcoded first\n",
    "ordinalData = [\"age\",\"tumor-size\",\"inv-nodes\"]\n",
    "for i in ordinalData:\n",
    "   columns = X[i].unique()\n",
    "   columns.sort() #sorting the labels\n",
    "   for j in range(len(columns)):\n",
    "      X.loc[X[i]==columns[j],i] = j\n",
    "y=read_file[[\"class\"]]\n",
    "runCBA(X,y,\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can look into whether there are any ordinal rules to encode\n",
    "read_file = pd.read_csv ('german.csv')\n",
    "read_file = read_file.dropna()\n",
    "X=read_file[[\"checkAccStatus\", \"durationMth\", \"credHist\", \"purpose\", \"credAmt\",\"savAccBond\",\"emplySince\",\"instRate\",\"personalStatSex\",\"otherDebtGuar\",\"presResSince\",\"prpty\",\"age(years)\",\"otherInstallPlans\",\"housing\",\"numExistCreds\",\"job\",\"numPplMaintain\",\"telephone\",\"frgnWorker\"]]\n",
    "for col in ['checkAccStatus','credHist','purpose','savAccBond','emplySince','personalStatSex','otherDebtGuar','prpty','otherInstallPlans','housing','job','telephone','frgnWorker']:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "y=read_file[[\"goodBad\"]]\n",
    "runCBA(X,y,\"goodBad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Student Placement (To classify as either placed or not placed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = pd.read_csv ('Placement_data_Full_Class.csv')\n",
    "read_file = read_file.dropna()\n",
    "X = read_file[['sl_no', 'gender', 'ssc_p', 'ssc_b', 'hsc_p', 'hsc_b', 'hsc_s', 'degree_p', 'degree_t', 'workex', 'etest_p', 'specialisation', 'mba_p', 'salary']]\n",
    "#creating a function to encode categorical features into numerical\n",
    "for col in ['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation']:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "y=read_file[['status']]\n",
    "runCBA(X,y,\"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke = stroke.dropna()\n",
    "X=stroke[[\"gender\", \"age\", \"heart_disease\", \"ever_married\", \"work_type\",\"Residence_type\",\"avg_glucose_level\",\"bmi\",\"smoking_status\"]]\n",
    "#encoding the normal data\n",
    "for col in [\"gender\",\"ever_married\",\"work_type\",\"Residence_type\"]:\n",
    "   X[col] = LabelEncoder().fit_transform(X[col])\n",
    "#encoding the ordinal data #Ordinal Encoder not working so i just hardcoded first\n",
    "ordinalData = [\"smoking_status\"]\n",
    "for i in ordinalData:\n",
    "   smokingStatus = [\"never smoked\",\"formerly smoked\",\"smokes\",] #need to think about how to encode unknown\n",
    "   smokingStatus = [\"Unknown\",\"never smoked\",\"formerly smoked\",\"smokes\"] #need to think about how to encode unknown\n",
    "   for j in range(len(smokingStatus)):\n",
    "      X.loc[X[i]==smokingStatus[j],i] = int(j)\n",
    "y=stroke[[\"stroke\"]]\n",
    "runCBA(X,y,\"stroke\") #for some reason this cannot work cuz it keep going out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.predict_probability(txns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassAssocationRule(Antecedent({}), Consequent(\"class\", \"default_class1\"), 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.rule_model_accuracy(txns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.rule_model_accuracy(txnstest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best association rules\n",
    "rules = top_rules(txns.string_representation)\n",
    "\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "predictor = M2Classi(cars, txns).build()\n",
    "\n",
    "\n",
    "accuracy = predictor.test_transactions(txnstest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = pd.read_csv ('heart.csv')\n",
    "read_file = read_file.dropna()\n",
    "X=read_file[ [\"age\", \"sex\", \"chest pain type\", \"resting blood pressure\", \"serum cholesterol (mg/dl)\", \"resting blood sugar >120mg/dl\",\"resting electrocariographic results\",\"maximum heart rate received\",\"exercise induced angina\", \"oldpeak\",\"slopePeak\", \"numMajorVessels\",\"thal\"]]\n",
    "y=read_file[[\"class\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=14, test_size=0.2, stratify = y)#25\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test=pd.concat([X_test, y_test], axis=1)\n",
    "txns = TransactionDB.from_DataFrame(train, target=\"class\") #../../\n",
    "txnstest = TransactionDB.from_DataFrame(test, target=\"class\") #../../\n",
    "\n",
    "cba = CBA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.fit(txns)\n",
    "cba.target_class\n",
    "cba.pre.rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.predict_probability(txns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassAssocationRule(Antecedent({}), Consequent(\"class\", \"default_class1\"), 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.rule_model_accuracy(txns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cba.rule_model_accuracy(txnstest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best association rules\n",
    "rules = top_rules(txns.string_representation)\n",
    "\n",
    "# convert them to class association rules\n",
    "cars = CARlist(rules)\n",
    "\n",
    "predictor = M2Classi(cars, txns).build()\n",
    "\n",
    "\n",
    "accuracy = predictor.test_transactions(txnstest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "initiate test for iris set (1).ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8a200d4ab7c7ebf76ea22aceaa977e32aafea98b990ef1dce2e1c4fa1dc871b8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
